{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find matches in each pair group (DM, DF), (DM, UM), (DF, UF), (UM, UF), hen analyzing matched of each pair group to get a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from scipy import linalg\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_dir = Path.cwd() / 'datasets/yelp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_dir = Path.cwd() / 'datasets/stackexchange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_dir = Path.cwd() / 'datasets/reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis_male = pd.read_csv('disclosed_male.csv',sep='|')\n",
    "# Load disclosed male\n",
    "dis_male = pd.read_csv(dataset_dir / 'disclosed_male_l_s_r.csv', sep='|', index_col=0)\n",
    "dis_male = dis_male.reset_index(drop=True)\n",
    "dis_male['Sentiment'] = dis_male.Sentiment.apply(lambda x: ast.literal_eval(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis_female = pd.read_csv('disclosed_female.csv',sep='|')\n",
    "# Load disclosed female\n",
    "dis_female = pd.read_csv(dataset_dir / 'disclosed_female_l_s_r.csv', sep='|', index_col=0)\n",
    "dis_female = dis_female.reset_index(drop=True)\n",
    "dis_female['Sentiment'] = dis_female.Sentiment.apply(lambda x: ast.literal_eval(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undis_male = pd.read_csv('undisclosed_male.csv',sep='|')\n",
    "# Load undisclosed male\n",
    "undis_male = pd.read_csv(dataset_dir / 'undisclosed_male_l_s_r.csv', sep='|', index_col=0)\n",
    "undis_male = undis_male.reset_index(drop=True)\n",
    "# undis_male.head()\n",
    "undis_male['Sentiment'] = undis_male.Sentiment.apply(lambda x: ast.literal_eval(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undis_male.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undis_female = pd.read_csv('undisclosed_female.csv',sep='|')\n",
    "# Load undisclosed female\n",
    "undis_female = pd.read_csv(dataset_dir / 'undisclosed_female_l_s_r.csv', sep='|', index_col=0)\n",
    "undis_female = undis_female.reset_index(drop=True)\n",
    "undis_female['Sentiment'] = undis_female.Sentiment.apply(lambda x: ast.literal_eval(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dis_male.shape, dis_female.shape, undis_male.shape, undis_female.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "def date_to_timestamp(date_str):\n",
    "    return time.mktime(datetime.fromisoformat(date_str).timetuple())\n",
    "\n",
    "# choosing one dataset\n",
    "dataset_str = 'yelp'\n",
    "if dataset_str == 'yelp':\n",
    "    dis_male.rename(columns={'categories': 'Categories'}, inplace=True)\n",
    "    dis_female.rename(columns={'categories': 'Categories'}, inplace=True)\n",
    "    undis_male.rename(columns={'categories': 'Categories'}, inplace=True)\n",
    "    undis_female.rename(columns={'categories': 'Categories'}, inplace=True)\n",
    "    usecols = ['stars','timestamp','Length','GradeLevel','Sentiment']\n",
    "elif dataset_str == 'stackexchange':\n",
    "    usecols = ['Reputation','Timestamp','Length','GradeLevel','Sentiment']\n",
    "    dis_female['Categories'] = 'computer science'\n",
    "    dis_male['Categories'] = 'computer science'\n",
    "    undis_female['Categories'] = 'computer science'\n",
    "    undis_male['Categories'] = 'computer science'\n",
    "    \n",
    "    undis_female['Timestamp'] = undis_female['Timestamp'].apply(lambda x: date_to_timestamp(x))\n",
    "    undis_male['Timestamp'] = undis_male['Timestamp'].apply(lambda x: date_to_timestamp(x))\n",
    "    dis_female['Timestamp'] = dis_female['Timestamp'].apply(lambda x: date_to_timestamp(x))\n",
    "    dis_male['Timestamp'] = dis_male['Timestamp'].apply(lambda x: date_to_timestamp(x))\n",
    "elif dataset_str == 'reddit':\n",
    "    usecols = ['Timestamp','Length','GradeLevel','Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahalanobis Distance\n",
    "def calculateCdist(df_1_s, df_2, cov_inv, usecols=[]):\n",
    "    cnt = df_2.shape[0]\n",
    "    slice_len = 250000\n",
    "    obtained_pairs = []\n",
    "    similarity_val = []\n",
    "    n = df_1_s.shape[0]\n",
    "    # control set too big, sliced in many steps\n",
    "    if cnt > slice_len:\n",
    "        slice_cnt = int(cnt / slice_len) + 1\n",
    "        for i in range(slice_cnt):\n",
    "            u_s = i * slice_len\n",
    "            \n",
    "            if i < slice_cnt - 1:\n",
    "                u_e = u_s + slice_len\n",
    "                df_2_s = df_2[u_s:u_e]\n",
    "            else:\n",
    "                df_2_s = df_2[u_s:]\n",
    "\n",
    "#             Y = distance.cdist(df_1_s[['stars', 'timestamp', 'length', 'Grade_level', 'Sentiment']], \\\n",
    "#                                df_2_s[['stars', 'timestamp', 'length', 'Grade_level', 'Sentiment']], \\\n",
    "#                                'mahalanobis', VI=cov_inv)\n",
    "            # 2 Dimensional array recording the distance of each pair between df_1_s and df_2_s\n",
    "            Y = distance.cdist(df_1_s[usecols], df_2_s[usecols], 'mahalanobis', VI=cov_inv)\n",
    "            # the sample from df_2_s closest to sample at a row \n",
    "            Y_1 = Y.argmin(axis=1)\n",
    "\n",
    "            # j: j-th sample from df_1_s\n",
    "            # Y_1[j]: column index of the closest sample from 2 \n",
    "            # Y[j, Y_1[j]]: the minimal distance of sample pair (j, Y_1[j])\n",
    "            # i: slice number, j: sample number in treatment set\n",
    "            min_y = [(Y_1[j], Y[j, Y_1[j]], i) for j in range(n)]\n",
    "            obtained_pairs.append(min_y)\n",
    "\n",
    "        # obtaining the matched set\n",
    "        matched_set = []\n",
    "        for i in range(n):\n",
    "            for ind, val, s in sorted(list(zip(*obtained_pairs))[i], key=lambda x:x[1]):\n",
    "                # s: slice number, val: minimal distance of sample i from treatment group in slice s\n",
    "                # ind: index of sample in control group to which smaple i from treatment group is closest in slice s\n",
    "                pos = s * slice_len + ind # real index in the entire control group\n",
    "                matched_set.append(pos)\n",
    "                similarity_val.append(val)\n",
    "                break # just consider the first (minimal) mathch\n",
    "        \n",
    "        return matched_set, similarity_val\n",
    "    else:\n",
    "        Y = distance.cdist(df_1_s[usecols], df_2[usecols], 'mahalanobis', VI=cov_inv)\n",
    "        Y_1 = Y.argmin(axis=1)\n",
    "        \n",
    "        for i in range(Y.shape[0]):\n",
    "            similarity_val.append(Y[i, Y_1[i]])\n",
    "        \n",
    "        return Y_1, similarity_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCLOSED_MALE = 0\n",
    "DISCLOSED_FEMALE = 1\n",
    "UNDISCLOSED_MALE = 2\n",
    "UNDISCLOSED_FEMALE = 3\n",
    "SIZE_MAX = 10000\n",
    "def findMatchAllPairs(d_m, d_f, u_m, u_f, sample_size=None):\n",
    "    total_size =  d_m.shape[0] + d_f.shape[0] + u_m.shape[0] + u_f.shape[0]\n",
    "    if sample_size is None:\n",
    "        sample_size = total_size\n",
    "    elif sample_size > total_size:\n",
    "        raise Exception('sample size beyond the number of population.')\n",
    "        \n",
    "    if total_size > SIZE_MAX:\n",
    "        print('total size over 100.000')\n",
    "        sample_size = SIZE_MAX\n",
    "    print('total size of samples: ', sample_size)\n",
    "    pairs = {}\n",
    "    \n",
    "    if 'group' not in d_m.columns:\n",
    "        d_m.insert(0,'group', DISCLOSED_MALE)\n",
    "    if 'group' not in d_f.columns:\n",
    "        d_f.insert(0,'group', DISCLOSED_FEMALE)\n",
    "    if 'group' not in u_m.columns:\n",
    "        u_m.insert(0,'group', UNDISCLOSED_MALE)\n",
    "    if 'group' not in u_f.columns:\n",
    "        u_f.insert(0,'group', UNDISCLOSED_FEMALE)\n",
    "    \n",
    "    all_data = pd.concat([d_m, d_f, u_m, u_f])\n",
    "    \n",
    "    if sample_size is None:\n",
    "        print('sample size is None, find match for all elements.')\n",
    "        saple_n = all_data\n",
    "    else:\n",
    "        sample_n = all_data.sample(sample_size)\n",
    "    \n",
    "    samp_0 = sample_n[sample_n['group'] == DISCLOSED_MALE]\n",
    "    samp_1 = sample_n[sample_n['group'] == DISCLOSED_FEMALE]\n",
    "    samp_2 = sample_n[sample_n['group'] == UNDISCLOSED_MALE]\n",
    "    samp_3 = sample_n[sample_n['group'] == UNDISCLOSED_FEMALE]\n",
    "    \n",
    "    print(f'sample DM - {len(samp_0)}, sample DF - {len(samp_1)}, \\\n",
    "    sample UM - {len(samp_2)}, sample UF - {len(samp_3)}')\n",
    "    \n",
    "    data = [(samp_0, DISCLOSED_MALE), (samp_1, DISCLOSED_FEMALE), (samp_2, UNDISCLOSED_MALE), (samp_3, UNDISCLOSED_FEMALE)]\n",
    "    \n",
    "    for i in range(3):\n",
    "        samp_treatment, label_treatment = data[i]\n",
    "        popln_treatment = all_data[all_data['group'] == label_treatment]\n",
    "        # rest groups\n",
    "        for j in range(i+1, 4):\n",
    "            samp_control, label_control = data[j] \n",
    "            popln_control = all_data[all_data['group'] == label_control]\n",
    "\n",
    "            pop_size = popln_treatment.shape[0] + popln_control.shape[0]\n",
    "            \n",
    "            if pop_size < 1000000:\n",
    "                m = pop_size\n",
    "            else:\n",
    "                m = 1000000\n",
    "\n",
    "            all_sample = pd.concat([popln_treatment, popln_control])\n",
    "            cov = np.cov(all_sample[usecols].sample(m).values, rowvar=False)\n",
    "            cov_inv = linalg.inv(cov)\n",
    "\n",
    "            print('covariance matrix obtained')\n",
    "            \n",
    "            # exmaple: 0-1\n",
    "            matched_indices_in_popln, similarity_1 = calculateCdist(samp_treatment, popln_control, cov_inv) \n",
    "            samp_entries = [samp_treatment.iloc[i] for i in range(samp_treatment.shape[0])]\n",
    "            popln_entries = [popln_control.iloc[i] for i in matched_indices_in_popln]\n",
    "            pairs[str(label_treatment) + '-' + str(label_control)] = list(zip(samp_entries, popln_entries)) \n",
    "            \n",
    "            # exmaple: 1-0\n",
    "            matched_indices_in_popln, similarity_2 = calculateCdist(samp_control, popln_treatment, cov_inv)\n",
    "            popln_entries = [popln_treatment.iloc[i] for i in matched_indices_in_popln]\n",
    "            samp_entries = [samp_control.iloc[i] for i in range(samp_control.shape[0])]\n",
    "            pairs[str(label_control) + '-' + str(label_treatment)] = list(zip(popln_entries, samp_entries))\n",
    "                   \n",
    "    return pairs  #,similarity_1,similarity_2                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a specific category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'restaurants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterCategory(x,category):\n",
    "    if category in x.lower():\n",
    "        return True\n",
    "    return False\n",
    "dis_male_c = dis_male[dis_male['categories'].apply(lambda x:filterCategory(x,category))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_male_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_female_c = dis_female[dis_female['categories'].apply(lambda x:filterCategory(x,category))]\n",
    "undis_male_c = undis_male[undis_male['categories'].apply(lambda x:filterCategory(x,category))]\n",
    "undis_female_c = undis_female[undis_female['categories'].apply(lambda x:filterCategory(x,category))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_female_c.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_all = findMatchAllPairs(dis_male_c,dis_female_c,undis_male_c,undis_female_c, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs_all\n",
    "pairs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating over all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterCategory(x,category):\n",
    "    if category in x.lower():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yelp\n",
    "import json\n",
    "dataset_str = 'yelp'\n",
    "if dataset_str == 'yelp':\n",
    "    top_10 = False\n",
    "    if top_10:\n",
    "        with open(dataset_dir / 'top_10_categories.json') as json_file:\n",
    "            categories = json.load(json_file)\n",
    "        categories = [x[0] for x in categories]\n",
    "    else:\n",
    "        with open(dataset_dir / 'top_5_indirect_categories.json') as json_file:\n",
    "            categories = json.load(json_file)\n",
    "elif dataset_str == 'stackexchange':\n",
    "    categories = ['computer science']\n",
    "elif dataset_str == 'reddit':\n",
    "    with open(dataset_dir / 'top_10_categories.json') as json_file:\n",
    "        categories = json.load(json_file)\n",
    "    categories = [x[0] for x in categories]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test test\n",
    "d_m_c_test = dis_male[dis_male['Categories'].apply(lambda x:filterCategory(x, \"men's hair salons\"))]\n",
    "d_f_c_test = dis_female[dis_female['Categories'].apply(lambda x:filterCategory(x, \"men's hair salons\"))]\n",
    "u_m_c_test = undis_male[undis_male['Categories'].apply(lambda x:filterCategory(x, \"men's hair salons\"))]\n",
    "u_f_c_test = undis_female[undis_female['Categories'].apply(lambda x:filterCategory(x, \"men's hair salons\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_m_c_test.shape, d_f_c_test.shape, u_m_c_test.shape, u_f_c_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_m_c_test.useful.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "d_f_c_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_m_c_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'group' not in dis_male.columns:\n",
    "    dis_male.insert(0, 'group', DISCLOSED_MALE)\n",
    "if 'group' not in dis_female.columns:\n",
    "    dis_female.insert(0, 'group', DISCLOSED_FEMALE)\n",
    "if 'group' not in undis_male.columns:\n",
    "    undis_male.insert(0, 'group', UNDISCLOSED_MALE)\n",
    "if 'group' not in undis_female.columns:\n",
    "    undis_female.insert(0, 'group', UNDISCLOSED_FEMALE)\n",
    "    \n",
    "# run n times \n",
    "sample_times = 1\n",
    "for i in range(0, sample_times): # one sample enough\n",
    "    for category in categories:\n",
    "        print(f'category - {category}')\n",
    "        \n",
    "        d_m_c = dis_male[dis_male['Categories'].apply(lambda x:filterCategory(x, category))]\n",
    "        d_f_c = dis_female[dis_female['Categories'].apply(lambda x:filterCategory(x, category))]\n",
    "        u_m_c = undis_male[undis_male['Categories'].apply(lambda x:filterCategory(x, category))]\n",
    "        u_f_c = undis_female[undis_female['Categories'].apply(lambda x:filterCategory(x, category))]\n",
    "\n",
    "        pairs = findMatchAllPairs(d_m_c, d_f_c, u_m_c, u_f_c)\n",
    "    \n",
    "        path = dataset_dir / ('category_pairs/sample_' + str(i)) \n",
    "        abs_file = path / (category + '.pickle')\n",
    "        if not path.exists():                                    \n",
    "            path.mkdir(parents=True)\n",
    "        with open(abs_file, 'wb+') as ft:\n",
    "            pickle.dump(pairs, ft)\n",
    "    \n",
    "    print(f'finished sample - {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_male.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advantageFunc(x,y):\n",
    "    return (x-y) / min([x,y]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMean(h):\n",
    "    return np.mean([x[0] for x in h]), np.mean([x[1] for x in h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_str == 'yelp':\n",
    "    attr = 'useful'\n",
    "elif dataset_str == 'stackexchange':\n",
    "    attr = ''\n",
    "elif dataset_str == 'reddit':\n",
    "    attr = 'Score'\n",
    "def getHelfulnessScore(pairs, keys):\n",
    "\n",
    "    h1 = [(m_1[attr], m_2[attr]) for m_1, m_2 in pairs[keys[0]]]\n",
    "    h2 = [(m_1[attr], m_2[attr]) for m_1, m_2 in pairs[keys[1]]]\n",
    "    return h1, h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMetric(pairs, key=None, bootstrap=False):\n",
    "    res = {}\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(i + 1, 4):\n",
    "            # i as treatment (sbuset), j as control (whole set)\n",
    "            key_1 = str(i) + '-' + str(j)\n",
    "            # j as treatment (subset), i as control (whole set)\n",
    "            key_2 = str(j) + '-' + str(i)\n",
    "            \n",
    "            h1, h2 = getHelfulnessScore(pairs, (key_1, key_2))\n",
    "            #total = h1 + h2\n",
    "            est = []\n",
    "            if bootstrap:\n",
    "                for _ in range(1000):\n",
    "                    total_set = h1 + h2 \n",
    "                    bootstrap_set = resample(total_set)\n",
    "                    h_s_1, h_s_2 = calculateMean(bootstrap_set)\n",
    "                    # perIncr\n",
    "                    est.append(advantageFunc(h_s_1, h_s_2))\n",
    "                # Compute standard error of the mean.\n",
    "                # Compute the standard deviation along the specified axis.\n",
    "                res[key_1] = (np.mean(est), stats.sem(est), np.std(est))\n",
    "            else:\n",
    "                h_s_1, h_s_2 = calculateMean(h1 + h2)\n",
    "                res[key_1] = advantageFunc(h_s_1, h_s_2)\n",
    "                \n",
    "    return res, est#,total   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_str == 'yelp':\n",
    "    if top_10:\n",
    "        cat_plot = ('restaurants', 'shopping', 'food', 'home services', \n",
    "        'beauty & spas', 'health & medical', 'local services', 'automotive', 'nightlife', 'bars', 'overall')\n",
    "    else:\n",
    "        cat_plot = ('sewing & alterations', 'self storage', 'carpet cleaning', \n",
    "                    'oral surgeons', \"men's hair salons\", 'restaurants', 'overall')\n",
    "elif dataset_str == 'stackexchange':\n",
    "    cat_plot = ('computer science', 'overall')\n",
    "elif dataset_str == 'reddit':\n",
    "    cat_plot = tuple(categories + ['overall'])\n",
    "    \n",
    "print(cat_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#mpl.style.use('classic')\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['figure.figsize'] =  12, 8\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['axes.linewidth'] = 2.5\n",
    "\n",
    "ind = np.arange(5)    \n",
    "width = 0.35 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(per_incr, grp_1, grp_2, std_err=None, dir_=''):\n",
    "    #plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    y_pos = np.arange(len(cat_plot))\n",
    "    \n",
    "    # deep: signaled\n",
    "    color_dict = {'SM': 'blue', 'SF': 'red', 'PM': '#9999ff', 'PF': '#ff9999'}\n",
    "    \n",
    "    hatch, color, fill = [], [], []\n",
    "    for i in per_incr:\n",
    "        if i < 0:\n",
    "            hatch.append('///')\n",
    "            color.append(color_dict[grp_2])\n",
    "            fill.append(True)\n",
    "        else:\n",
    "            hatch.append('///')\n",
    "            color.append(color_dict[grp_1])\n",
    "            fill.append(True)\n",
    "\n",
    "    legend_patch_1 = Patch(fill=True, label=grp_1, hatch='///', color=color_dict[grp_1])\n",
    "    legend_patch_2 = Patch(fill=True, label=grp_2, hatch='///', color=color_dict[grp_2])\n",
    "            \n",
    "    \n",
    "    barlist = ax.barh(y_pos, per_incr, height=0.6, align='center', xerr=std_err)\n",
    "    for i,thisbar in enumerate(barlist.patches):\n",
    "        thisbar.set_hatch(hatch[i])\n",
    "        thisbar.set_color(color[i])\n",
    "        thisbar.set_fill(fill[i])\n",
    "    \n",
    "    majorLocator = MultipleLocator(20)\n",
    "    majorFormatter = FormatStrFormatter('%d')\n",
    "    minorLocator = MultipleLocator(2.5)\n",
    "\n",
    "    ax.xaxis.set_major_locator(majorLocator)\n",
    "    ax.xaxis.set_major_formatter(majorFormatter)\n",
    "\n",
    "    # for the minor ticks, use no labels; default NullFormatter\n",
    "    ax.xaxis.set_minor_locator(minorLocator)\n",
    "    ax.set_yticks(y_pos)\n",
    "    # +1 reserving some space at the bottom for placing legend\n",
    "    plt.ylim(-1, len(per_incr) + 1)\n",
    "    ax.set_yticklabels(cat_plot)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Advantage (%)')\n",
    "#     plt.xlim(-50,50)\n",
    "#     x_pos = np.arange(-50, 60, step=10)\n",
    "    plt.xlim(-100, 100)\n",
    "#     xticklabels = [50, 40, 30, 20, 10, 0, 10, 20, 30, 40, 50]\n",
    "    xticklabels = [100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    x_pos = np.arange(-100, 110, step=10)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    # plot a vertical line x=0\n",
    "    plt.axvline(0, color='black')\n",
    "#     plt.axhline(14.58, color='black')\n",
    "#     plt.text(-40, 0, grp_1, fontsize=20)\n",
    "#     plt.text(35, 0, grp_2, fontsize=20)\n",
    "    plt.grid(linestyle='--')\n",
    "    # overall label color\n",
    "    ax.get_yticklabels()[-1].set_color(\"r\")\n",
    "    title = grp_1 + '_' + grp_2 + '.jpg'\n",
    "    plt.tight_layout()\n",
    "    plt.legend(handles = [legend_patch_1, legend_patch_2], loc='lower right')\n",
    "    plt.savefig(dir_ / title, dpi=250)\n",
    "    #plt.savefig('plotNatural/'+title, format='svg', dpi=1200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# categories = ['Books','Electronics','CDs & Vinyl','Clothing, Shoes & Jewelry','Home & Kitchen',\\\n",
    "#              'Kindle Store','Sports & Outdoors','Cell Phones & Accessories', 'Toys & Games','Games','Literature & Fiction',\\\n",
    "#              'Beauty','Health & Personal Care','Movies','Computers']\n",
    "\n",
    "all_res = []\n",
    "sample_times = 1\n",
    "for i in range(0, sample_times):\n",
    "    all_results = {}\n",
    "    for category in categories:\n",
    "        bootstrap_res = {}\n",
    "        path = dataset_dir / ('category_pairs/sample_' + str(i) + '/' + category + '.pickle')\n",
    "        with open(path, 'rb') as fs:\n",
    "            pairs = pickle.load(fs)\n",
    "        res, _ = calculateMetric(pairs, bootstrap=True)\n",
    "        # scipy.stats.sem\n",
    "        all_results[category] = res\n",
    "\n",
    "        print(f'done for {category}')\n",
    "    \n",
    "    print(f'sample {i} finished')\n",
    "    all_res.append(all_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['restaurants'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if STACKEXCHANGE:\n",
    "#     categories = ['computer science']\n",
    "# compute overall mean and std_err (0)\n",
    "def getResults(res, key):\n",
    "    val = []\n",
    "    std_err = []\n",
    "    for category in categories:\n",
    "        val.append(res[category][key][0])\n",
    "        std_err.append(res[category][key][1])\n",
    "\n",
    "    # overall mean\n",
    "    mean = np.mean(val)\n",
    "    val.append(mean)\n",
    "    std_err.append(0)\n",
    "    return val, std_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '0-1', '0-2', '2-3', '1-3'\n",
    "# DISCLOSED_MALE = 0\n",
    "# DISCLOSED_FEMALE = 1\n",
    "# UNDISCLOSED_MALE = 2\n",
    "# UNDISCLOSED_FEMALE = 3\n",
    "# all_res[0]: 0 denotes the sample round\n",
    "# 0-1: DM DF, 0-2: DM UM, 2-3: UM UF, 1-3: DF UF\n",
    "val, std_err = getResults(all_res[0], '0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val, std_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM: Performative Male (undisclosed), SM: Singnal Male (disclosed)\n",
    "val, std_err = getResults(all_res[0], '0-1')\n",
    "plotResults(val, 'SM', 'SF', std_err=std_err, dir_=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, std_err = getResults(all_res[0], '0-2')# 0-1: DM DF, 0-2: DM UM, 2-3: UM UF, 1-3: DF UF\n",
    "plotResults(val, 'SM', 'PM', std_err=std_err, dir_=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, std_err = getResults(all_res[0], '1-3')# 0-1: DM DF, 0-2: DM UM, 2-3: UM UF, 1-3: DF UF\n",
    "plotResults(val, 'SF', 'PF', std_err=std_err, dir_=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, std_err = getResults(all_res[0], '2-3')# 0-1: DM DF, 0-2: DM UM, 2-3: UM UF, 1-3: DF UF\n",
    "plotResults(val, 'PM', 'PF', std_err=std_err, dir_=dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [deprecated] Calculate covariate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Books','Electronics','CDs & Vinyl','Clothing, Shoes & Jewelry','Home & Kitchen',\\\n",
    "             'Kindle Store','Sports & Outdoors','Cell Phones & Accessories', 'Toys & Games','Games','Literature & Fiction',\\\n",
    "             'Beauty','Health & Personal Care','Movies','Computers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Books'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'category_pairs/sample_3/' + category + '.pickle'\n",
    "with open(path,'rb') as fs:\n",
    "    pairs = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs['1-0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDistribution(lst):\n",
    "    u_el = list(set(lst))\n",
    "    u_el.sort()\n",
    "    dist = []\n",
    "    for el in u_el:\n",
    "        el_cnt = lst.count(el)\n",
    "        dist.append(el_cnt)\n",
    "    s = sum(dist)\n",
    "    return u_el, [i/s for i in dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCovariateHist(pairs,x_label=None,covariate=None):\n",
    "    keys = ['0-1', '1-0', '2-3', '3-2', '1-3', '3-1', '0-2', '2-0']\n",
    "    \n",
    "    confounder = {}\n",
    "    \n",
    "    for key in keys:\n",
    "        for pair in pairs[key]:\n",
    "            x = pair[0]['group']\n",
    "            val_x = pair[0][covariate]\n",
    "            y = pair[1]['group']\n",
    "            val_y = pair[1][covariate]\n",
    "            if x not in confounder:\n",
    "                confounder[x] = []\n",
    "            if y not in confounder:\n",
    "                confounder[y] = []\n",
    "            confounder[x].append(val_x)\n",
    "            confounder[y].append(val_y)\n",
    "    \n",
    "    \n",
    "    confounder_fil = {}\n",
    "    confounder_fil[0] = [i for i in confounder[0] if i in range(1,16)]\n",
    "    confounder_fil[1] = [i for i in confounder[1] if i in range(1,16)]\n",
    "    confounder_fil[2] = [i for i in confounder[2] if i in range(1,16)]\n",
    "    confounder_fil[3] = [i for i in confounder[3] if i in range(1,16)]\n",
    "    \n",
    "    print(min(confounder_fil[0]),max(confounder_fil[0]))\n",
    "    print(min(confounder_fil[1]),max(confounder_fil[1]))\n",
    "    print(min(confounder_fil[2]),max(confounder_fil[2]))\n",
    "    print(min(confounder_fil[3]),max(confounder_fil[3]))\n",
    "    \n",
    "    width = 0.15\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['b','r','b','r']\n",
    "    edge_colors = [None,None,'b','r']\n",
    "    #hatches = ['','','xxx','xxx']\n",
    "    linestyles = ['-','-','--','--']\n",
    "    fills = [True,True,False,False]\n",
    "    labels = ['SM','SW','PM','PW']\n",
    "    #x_pos = np.arange(1.5,60,step=10)\n",
    "    #ax.set_xticks(x_pos)\n",
    "    #ax.set_xticklabels([50,40,30,20,10,0,10,20,30,40,50])\n",
    "    #print(min(confounder[0]),min(confounder[1]),min(confounder[2]),min(confounder[3]))\n",
    "    for i in range(4):\n",
    "        x,dist = createDistribution(confounder_fil[i])\n",
    "        plt.plot(x,dist,color = colors[i],label = labels[i],linewidth=2,linestyle = linestyles[i])\n",
    "        #x = [i*width+j for j in x]\n",
    "        #plt.bar(x,dist,color=colors[i],hatch=hatches[i],fill=fills[i],label=labels[i],width=0.15,edgecolor=edge_colors[i])\n",
    "    \n",
    "    index = np.arange(1,16)\n",
    "    plt.xticks(index + width*1.5, (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('PMF')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plotCovariates/'+covariate+'.jpg',dpi=250)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = calcCovariateHist(pairs,x_label='Readability',covariate='Grade_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCovariateDist(pairs,t,x_label='Readability',covariate='Grade_level'):\n",
    "    \n",
    "    flag = 0\n",
    "    dists_all = []\n",
    "    for i in range(4):\n",
    "        if i!=t:\n",
    "            if flag==0:\n",
    "                grp_0_1 = [(m[covariate],n[covariate]) for m,n in pairs[str(t)+'-'+str(i)]]\n",
    "                #grp_1_0 = [(n[covariate],m[covariate]) for m,n in pairs[str(i)+'-'+str(t)]]\n",
    "                #grp = grp_0_1 + grp_1_0\n",
    "                dist = list(zip(*grp_0_1))\n",
    "            \n",
    "                dist_0 = list(dist[0])\n",
    "                dists_all.append(list(dist[1]))\n",
    "            \n",
    "            else:\n",
    "                dists_all.append([n[covariate] for m,n in pairs[str(t)+'-'+str(i)]])\n",
    "                \n",
    "            #val,p = ks_2samp(dist_0,dist_1)\n",
    "            #print(val,p)\n",
    "            #dist_0.sort()\n",
    "            #dist_1.sort()\n",
    "            #y_1 = np.cumsum(dist_0)\n",
    "            #y_2 = np.cumsum(dist_1)\n",
    "            \n",
    "    #y,binEdges=np.histogram(dist_0,bins=100)\n",
    "    #bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "    #plt.plot(bincenters,y,'-',linewidth=2)\n",
    "    sns.distplot(dist_0,hist=False,rug=True,label='DM',kde_kws={'color':'b','linewidth':2})\n",
    "    \n",
    "    colors = ['r','b','r']\n",
    "    ls = ['-','--','--']\n",
    "    labels = ['DW','UM','UW']\n",
    "    \n",
    "    for i,dist in enumerate(dists_all):\n",
    "\n",
    "        #y,binEdges=np.histogram(dist,bins=100)\n",
    "        #bincenters = 0.5*(binEdges[1:]+binEdges[:-1])\n",
    "        #plt.plot(bincenters,y,'-',linewidth=2)\n",
    "        sns.distplot(dist,hist=False,rug=True,label=labels[i],\n",
    "                     kde_kws={'color':colors[i],'linestyle':ls[i],'linewidth':2})\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('PDF')\n",
    "    plt.grid(linestyle='--')\n",
    "    #plt.xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plotCovariates/'+covariate+'.jpg',dpi=250)\n",
    "    \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs['0-1'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcCovariateDist(pairs,0,x_label='Rating',covariate='Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_m_s = dis_male.sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_m_s = undis_male.sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_f_s = dis_female.sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_f_s = undis_female.sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kruskal(d_m_s['overall_sentiment'],u_m_s['overall_sentiment'],d_f_s['overall_sentiment'],u_f_s['overall_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#mpl.style.use('classic')\n",
    "mpl.rcParams['xtick.labelsize'] = 25\n",
    "mpl.rcParams['ytick.labelsize'] = 32\n",
    "mpl.rcParams['font.size'] = 20\n",
    "mpl.rcParams['figure.figsize'] =  9,10\n",
    "mpl.rcParams['axes.labelsize'] = 35\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['axes.linewidth'] = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcCovariateDist(pairs,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matches produced, only ploting once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path.cwd() / 'datasets/yelp'\n",
    "dataset_str = 'yelp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path.cwd() / 'datasets/reddit'\n",
    "dataset_str = 'reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yelp\n",
    "import json\n",
    "top_10 = False\n",
    "if dataset_str == 'yelp':\n",
    "    if top_10:\n",
    "        with open(dataset_dir / 'top_10_categories.json') as json_file:\n",
    "            categories = json.load(json_file)\n",
    "        categories = [x[0] for x in categories]\n",
    "    else:\n",
    "        with open(dataset_dir / 'top_5_indirect_categories.json') as json_file:\n",
    "            categories = json.load(json_file)\n",
    "elif dataset_str == 'stackexchange':\n",
    "    categories = ['computer science']\n",
    "elif dataset_str == 'reddit':\n",
    "    with open(dataset_dir / 'top_10_categories.json') as json_file:\n",
    "        categories = json.load(json_file)\n",
    "    categories = [x[0] for x in categories]\n",
    "print(categories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advantageFunc(x,y):\n",
    "    return (x-y) / min([x,y]) * 100\n",
    "\n",
    "def calculateMean(h):\n",
    "    return np.mean([x[0] for x in h]), np.mean([x[1] for x in h])\n",
    "\n",
    "if dataset_str == 'yelp':\n",
    "    attr = 'useful'\n",
    "elif dataset_str == 'stackexchange':\n",
    "    attr = ''\n",
    "elif dataset_str == 'reddit':\n",
    "    attr = 'Score'\n",
    "    \n",
    "def getHelfulnessScore(pairs, keys):\n",
    "    h1 = [(m_1[attr], m_2[attr]) for m_1, m_2 in pairs[keys[0]]]\n",
    "    h2 = [(m_1[attr], m_2[attr]) for m_1, m_2 in pairs[keys[1]]]\n",
    "    return h1, h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMetric(pairs, key=None, bootstrap=False):\n",
    "    res = {}\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(i + 1, 4):\n",
    "            # i as treatment (sbuset), j as control (whole set)\n",
    "            key_1 = str(i) + '-' + str(j)\n",
    "            # j as treatment (subset), i as control (whole set)\n",
    "            key_2 = str(j) + '-' + str(i)\n",
    "            \n",
    "            h1, h2 = getHelfulnessScore(pairs, (key_1, key_2))\n",
    "            #total = h1 + h2\n",
    "            est = []\n",
    "            if bootstrap:\n",
    "                for _ in range(1000):\n",
    "                    # append\n",
    "                    total_set = h1 + h2 \n",
    "                    bootstrap_set = resample(total_set)\n",
    "                    h_s_1, h_s_2 = calculateMean(bootstrap_set)\n",
    "                    # perIncr\n",
    "                    est.append(advantageFunc(h_s_1, h_s_2))\n",
    "                # Compute standard error of the mean.\n",
    "                # Compute the standard deviation along the specified axis.\n",
    "                res[key_1] = (np.mean(est), stats.sem(est), np.std(est))\n",
    "            else:\n",
    "                h_s_1, h_s_2 = calculateMean(h1 + h2)\n",
    "                res[key_1] = advantageFunc(h_s_1, h_s_2)\n",
    "                \n",
    "    return res, est#,total   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_str == 'yelp':\n",
    "    if top_10:\n",
    "        cat_plot = ('restaurants', 'shopping', 'food', 'home services', \n",
    "        'beauty & spas', 'health & medical', 'local services', 'automotive', 'nightlife', 'bars', 'overall')\n",
    "    else:\n",
    "        cat_plot = ('sewing & alterations', 'self storage', 'carpet cleaning', \n",
    "                    'oral surgeons', \"men's hair salons\", 'restaurants', 'overall')\n",
    "elif dataset_str == 'stackexchange':\n",
    "    cat_plot = ('computer science', 'overall')\n",
    "elif dataset_str == 'reddit':\n",
    "    cat_plot = tuple(categories + ['overall'])\n",
    "    \n",
    "print(cat_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#mpl.style.use('classic')\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['figure.figsize'] =  12, 8\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['axes.linewidth'] = 2.5\n",
    "\n",
    "ind = np.arange(5)    \n",
    "width = 0.35 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(per_incr, grp_1, grp_2, std_err=None, dir_=''):\n",
    "    #plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    y_pos = np.arange(len(cat_plot))\n",
    "    \n",
    "    # deep: signaled\n",
    "    color_dict = {'SM': 'blue', 'SW': 'red', 'PM': '#9999ff', 'PW': '#ff9999'}\n",
    "    \n",
    "    hatch, color, fill = [], [], []\n",
    "    for i in per_incr:\n",
    "        if i < 0:\n",
    "            hatch.append('///')\n",
    "            color.append(color_dict[grp_2])\n",
    "            fill.append(True)\n",
    "        else:\n",
    "            hatch.append('///')\n",
    "            color.append(color_dict[grp_1])\n",
    "            fill.append(True)\n",
    "\n",
    "    legend_patch_1 = Patch(fill=True, label=grp_1, hatch='///', color=color_dict[grp_1])\n",
    "    legend_patch_2 = Patch(fill=True, label=grp_2, hatch='///', color=color_dict[grp_2])\n",
    "            \n",
    "    \n",
    "    barlist = ax.barh(y_pos, per_incr, height=0.6, align='center', xerr=std_err)\n",
    "    for i,thisbar in enumerate(barlist.patches):\n",
    "        thisbar.set_hatch(hatch[i])\n",
    "        thisbar.set_color(color[i])\n",
    "        thisbar.set_fill(fill[i])\n",
    "    \n",
    "    majorLocator = MultipleLocator(20)\n",
    "    majorFormatter = FormatStrFormatter('%d')\n",
    "    minorLocator = MultipleLocator(2.5)\n",
    "\n",
    "    ax.xaxis.set_major_locator(majorLocator)\n",
    "    ax.xaxis.set_major_formatter(majorFormatter)\n",
    "\n",
    "    # for the minor ticks, use no labels; default NullFormatter\n",
    "    ax.xaxis.set_minor_locator(minorLocator)\n",
    "    ax.set_yticks(y_pos)\n",
    "    # +1 reserving some space at the bottom for placing legend\n",
    "    plt.ylim(-1, len(per_incr) + 1)\n",
    "    ax.set_yticklabels(cat_plot)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Advantage (%)')\n",
    "#     plt.xlim(-50,50)\n",
    "#     x_pos = np.arange(-50, 60, step=10)\n",
    "    plt.xlim(-100, 100)\n",
    "#     xticklabels = [50, 40, 30, 20, 10, 0, 10, 20, 30, 40, 50]\n",
    "    xticklabels = [100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    x_pos = np.arange(-100, 110, step=10)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    # plot a vertical line x=0\n",
    "    plt.axvline(0, color='black')\n",
    "#     plt.axhline(14.58, color='black')\n",
    "#     plt.text(-40, 0, grp_1, fontsize=20)\n",
    "#     plt.text(35, 0, grp_2, fontsize=20)\n",
    "    plt.grid(linestyle='--')\n",
    "    # overall label color\n",
    "    ax.get_yticklabels()[-1].set_color(\"r\")\n",
    "    title = grp_1 + '_' + grp_2 + '.jpg'\n",
    "    plt.tight_layout()\n",
    "    plt.legend(handles = [legend_patch_1, legend_patch_2], loc='lower right')\n",
    "    plt.savefig(dir_ / title, dpi=250)\n",
    "    #plt.savefig('plotNatural/'+title, format='svg', dpi=1200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "sample_times = 1\n",
    "for i in range(0, sample_times):\n",
    "    all_results = {}\n",
    "    for category in categories:\n",
    "        bootstrap_res = {}\n",
    "        path = dataset_dir / ('category_pairs/sample_' + str(i) + '_matchsize100000/' + category + '.pickle')\n",
    "        with open(path, 'rb') as fs:\n",
    "            pairs = pickle.load(fs)\n",
    "        res, _ = calculateMetric(pairs, bootstrap=True)\n",
    "        # scipy.stats.sem\n",
    "        all_results[category] = res\n",
    "\n",
    "        print(f'done for {category}')\n",
    "    \n",
    "    print(f'sample {i} finished')\n",
    "    all_res.append(all_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute overall mean and std_err (0)\n",
    "def getResults(res, key):\n",
    "    val = []\n",
    "    std_err = []\n",
    "    for category in categories:\n",
    "        val.append(res[category][key][0])\n",
    "        std_err.append(res[category][key][1])\n",
    "\n",
    "    # overall mean\n",
    "    mean = np.mean(val)\n",
    "    val.append(mean)\n",
    "    std_err.append(0)\n",
    "    return val, std_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, std_err = getResults(all_res[0], '0-1')\n",
    "plotResults(val, 'SM', 'SW', std_err=std_err, dir_=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, std_err = getResults(all_res[0], '0-2')# 0-1: DM DF, 0-2: DM UM, 2-3: UM UF, 1-3: DF UF\n",
    "plotResults(val, 'SM', 'PM', std_err=std_err, dir_=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, std_err = getResults(all_res[0], '1-3')# 0-1: DM DF, 0-2: DM UM, 2-3: UM UF, 1-3: DF UF\n",
    "plotResults(val, 'SW', 'PW', std_err=std_err, dir_=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
