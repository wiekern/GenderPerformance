{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4942a32d853e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'###'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "lstm_cell = nn.LSTMCell(128, 128)\n",
    "x = torch.randn(1, 128)\n",
    "make_dot(lstm_cell(x), params=dict(list(lstm_cell.named_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded vocabulary\n",
      "size of vocabulary:  109619\n",
      "Encoded reviews:  100000\n",
      "Encoded reviews:  100000\n",
      "created batches from data loader\n",
      "Encoder(\n",
      "  (embedding): Embedding(109620, 200, padding_idx=0)\n",
      "  (e2i): Linear(in_features=200, out_features=250, bias=True)\n",
      "  (gru): GRU(250, 250, num_layers=2, batch_first=True)\n",
      "  (dense1): Linear(in_features=250, out_features=250, bias=True)\n",
      "  (dense2): Linear(in_features=250, out_features=250, bias=True)\n",
      "  (out): Linear(in_features=250, out_features=2, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "True\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-df578718425f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'RNN_vanilla_2_yafei.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-df578718425f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, dataset_train, dataset_validate, batch_size, saveas, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_cnt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# a batch implementation of basic RNN model\n",
    "# vocab is filtered based on the frequency of words... only use words with frequency at least 5 ... replace others with unk token\n",
    "# pre-trained embeddings not used...\n",
    "# embeddings learnt on the go...\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.nn.utils import rnn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import preprocess as pp\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "\n",
    "import tensorwatch as tw\n",
    "from torchviz import make_dot\n",
    "from pathlib import Path\n",
    "#data_dir = Path.home() / 'GenderPerformance/datasets/reddit'\n",
    "data_dir = Path.home() / 'GenderPerformance/datasets/yelp'\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, reviews, labels, lengths):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = torch.tensor(self.reviews[index],dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[index],dtype=torch.long)\n",
    "        s_lengths = self.lengths[index]\n",
    "        return X,y,s_lengths\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_size, hidden_size, output_size, layers, padding_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoding_size = encoding_size\n",
    "        self.layers = layers\n",
    "        self.embedding = nn.Embedding(input_size, encoding_size, padding_idx=padding_idx)\n",
    "        self.e2i = nn.Linear(encoding_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=self.layers)\n",
    "        self.dense1 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.dense2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.batch_first = True\n",
    "\n",
    "    def forward(self, X, X_lengths, batch_size):\n",
    "\n",
    "        self.hidden = self.initHidden(batch_size)\n",
    "        X = self.embedding(X)\n",
    "        X = self.e2i(X)\n",
    "        X = rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "\n",
    "        X, self.hidden = self.gru(X, self.hidden)\n",
    "\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        idx = (torch.cuda.LongTensor(X_lengths) - 1).view(-1, 1).expand(len(X_lengths), X.size(2))\n",
    "\n",
    "        time_dimension = 1 if self.batch_first else 0\n",
    "        idx = idx.unsqueeze(time_dimension)\n",
    "        X = X.gather(time_dimension, Variable(idx)).squeeze(time_dimension)\n",
    "\n",
    "        X = self.dense1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.out(X)\n",
    "        X = self.sigmoid(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "         return torch.zeros(self.layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "def sortbylength(X,y,s_lengths):\n",
    "    sorted_lengths, indices = torch.sort(s_lengths,descending=True)\n",
    "    return X[torch.LongTensor(indices),:],y[torch.LongTensor(indices)],sorted_lengths\n",
    "\n",
    "\n",
    "def ValidationAccuracy(encoder,dataset_validate,batch_size):\n",
    "    loader_validate = data.DataLoader(dataset_validate,batch_size=batch_size)\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    for X,y,X_lengths in loader_validate:\n",
    "        X,y,X_lengths = sortbylength(X,y,X_lengths)\n",
    "        X,y,X_lengths = X.to(device),y.to(device),X_lengths.to(device)\n",
    "        b_size = y.size(0)\n",
    "        output = encoder(X,X_lengths,b_size)\n",
    "        output = F.softmax(output,dim=1)\n",
    "        value,labels = torch.max(output,1)\n",
    "\n",
    "        true_labels+=y.cpu().numpy().tolist()\n",
    "        predicted_labels+=labels.cpu().numpy().tolist()\n",
    "    \n",
    "    print(confusion_matrix(true_labels, predicted_labels))\n",
    "    return accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "\n",
    "def train(encoder, dataset_train, dataset_validate, batch_size, saveas = '', epochs=10, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss() # cross entropy loss in pytorch combines logsoftmax and negativeloglikelihoodloss...softmax layer not needed\n",
    "    validation_accuracy = 0\n",
    "    for i in range(epochs):\n",
    "        batch_cnt = 0\n",
    "        loader_train = data.DataLoader(dataset_train,batch_size=batch_size,shuffle=True)\n",
    "        for X,y,X_lengths in loader_train:\n",
    "            batch_cnt+=1\n",
    "            X,y,X_lengths = sortbylength(X,y,X_lengths)\n",
    "            X,y,X_lengths = X.to(device),y.to(device),X_lengths.to(device)\n",
    "            b_size = y.size(0)\n",
    "            output = encoder(X,X_lengths,b_size)\n",
    "#             tw.draw_model(output, [1, 3, 224, 224])\n",
    "            print(output.requires_grad)\n",
    "            make_dot(output)\n",
    "            assert False\n",
    "            loss = criterion(output,y)\n",
    "            if batch_cnt%100==0:\n",
    "                print('epoch - {}, batch count - {}, loss - {}'.format(i, batch_cnt, loss))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        accuracy = ValidationAccuracy(encoder,dataset_validate,batch_size)\n",
    "        print('accuracy: {} at epoch: {}'.format(accuracy,i))\n",
    "        if validation_accuracy < accuracy:\n",
    "            validation_accuracy = accuracy\n",
    "            torch.save(encoder.state_dict(), saveas)\n",
    "\n",
    "\n",
    "\n",
    "def sentence2tensor(sentence,w2i,pad,sent_length):\n",
    "    S = [w2i[w] for w in sentence]\n",
    "    if len(S)>sent_length:\n",
    "        return S[:sent_length]\n",
    "    else:\n",
    "        x = len(S)\n",
    "        S = S + [pad for i in range(sent_length - x)]\n",
    "        return S\n",
    "\n",
    "\n",
    "def encodeDataset(fname,w2i,padding_idx,sent_length):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    count = 0\n",
    "\n",
    "    filtered_fname = fname.with_name(fname.name + '_filtered')\n",
    "    with open(filtered_fname) as fs:    \n",
    "         for line in fs:\n",
    "             count+=1\n",
    "             label = line[0]\n",
    "             review = line[2:]\n",
    "             #words = review.strip().replace('.','').split()\n",
    "             words = word_tokenize(review.strip())\n",
    "             words = [w for w in words if w.isalpha()]\n",
    "             if len(words)>0:\n",
    "                reviews.append(sentence2tensor(words,w2i,padding_idx,sent_length))\n",
    "                if len(words)>sent_length:\n",
    "                    lengths.append(sent_length)\n",
    "                else:\n",
    "                    lengths.append(len(words))\n",
    "                labels.append(int(label))\n",
    "                if count%100000==0:\n",
    "                    print('Encoded reviews: ',count)\n",
    "                    break;\n",
    "\n",
    "    return reviews,labels,lengths\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    sent_length = 100\n",
    "    train_file_path = data_dir / 'training_gender_text.csv'\n",
    "    validation_file_path = data_dir / 'validation_gender_text.csv'\n",
    "    with open(data_dir / 'word2index.pickle','rb') as fs:\n",
    "        w2i = pickle.load(fs)\n",
    "\n",
    "    print('loaded vocabulary')\n",
    "    print('size of vocabulary: ', len(w2i))\n",
    "\n",
    "    vocab_size = len(w2i)\n",
    "    padding_idx = 0 \n",
    "\n",
    "    reviews_train,labels_train,lengths_train = encodeDataset(train_file_path,w2i,padding_idx,sent_length)\n",
    "    reviews_validate, labels_validate, lengths_validate = encodeDataset(validation_file_path,w2i,padding_idx,sent_length)\n",
    "    #reviews_test, labels_test, lengths_test = encodeDataset(test_file,w2i,padding_idx,sent_length)\n",
    "    print('created batches from data loader')\n",
    "\n",
    "\n",
    "    #print(reviews[2])\n",
    "    dataset_train = Dataset(reviews_train,labels_train,lengths_train)\n",
    "    dataset_validate = Dataset(reviews_validate,labels_validate,lengths_validate)\n",
    "    encoding_size = 200\n",
    "\n",
    "    hidden_size = 250\n",
    "    input_size = vocab_size+1\n",
    "    output_size = 2\n",
    "    layers = 2\n",
    "    batch_size = 256\n",
    "    encoder = Encoder(input_size, encoding_size, hidden_size, output_size,layers, padding_idx)\n",
    "#     tw.draw_model(encoder(X_lengths=32, batch_size=256), [1, 3, 224, 224])\n",
    "    print(encoder)\n",
    "    encoder = encoder.to(device)\n",
    "    train(encoder,dataset_train, dataset_validate, batch_size, saveas=data_dir / 'RNN_vanilla_2_yafei.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
